### 学习规划
- Linux Cluster主要两类:
    - LB集群: Load Balancing,负载均衡
    - HA集群: High Availability,高可用
    - HP集群: High Permormance,大规模并行处理分析平台;分布式存储及并行处理集群
- WEB Arch: web的高级应用
- 虚拟化和云计算
- 自动化运维工具:ansible,puppet,zabbix
- 大数据处理平台: hadoop 2, storm, spark, zookeeper
- PaaS,ELK

##### LB集群，负载均衡
- 集群: Cluster，将多台主机组织起来，处理统一问题.
- 系统扩展的思路:
    - scale up: 向上扩展,用性能更好的主机替代当前主机,消耗的代价和带来的性能提升是不成正比的.
    - scale out: 向外扩展,当主机难以负载现有的需求时,我们就加主机,把请求分散到诸多主机上;向外扩展其实面临很多问题

##### HA集群,高可用
- `衡量一个系统的可用性(Avalability)=无故障时间/(无故障时间+修复时间)`

##### 还有另外一种常见的集群，大规模并行处理分析平台;分布式存储及并行处理集群
- mapreduce：映射缩减；比如hadoop

#### 系统
- 衡量一个系统通过以下几个方面:
    - 可扩展性
    - 可用性
        - 服务是否随时可用
        - 容量的衡量：比如高速公路上，在多大密度下，可以通过多少量车
        - 性能：可以理解为单位时间内的通过量
    - 系统运维:对大量主机组成的系统来讲, `可用 --> 标准化 --> 自动化`
- 构建高可扩展性系统的重要原则: 在系统内部尽量避免串行化
- GSLB: Global Service Load Balancing; 在多城市,多机房部署负载均衡系统. SLB：本地负载均衡系统
- 缓存都是以KV(key value)结构存储的,有可能存储在文件系统上,键比对速度会非常快.
- 总结：
    - 分层：`负载均衡层 --> 静态内容层 --> 动态内容层 --> 数据存储层` 让没一个服务器各司其职
    - 分割: 如果一个系统把所有功能糅合在一块,它会变得很复杂;我们可以把不同应用分割到不同的服务器组上
    - 分布式: 分不了层的我们就可以做分布式
        - 分布式应用
        - 分布式静态资源
        - 分布式数据和存储
        - 分布式计算(hadoop)

### LVS基础

#### LB集群的实现,负载均衡

- 硬件:
    - F5:BIG-IP;有时候直接称为F5
    - Citrix NetScaler 思杰
    - Array
    - Redware

- 软件
    - lvs
    - haproxy
    - nginx
    - httpd
    - vanish
    - ats(apache traffic server)
    - perlbal

- 基于工作的协议层次划分:
    - 传输层: lvs, haproxy(可以模拟在tcp层)
    - 应用层: haproxy, nginx, ats, perlbal

##### lvs: linux virtual server

- lvs的作者是章文嵩,lvs是作为内核模块来工作的,所以我们可以把它编译进内核.全称叫做linux virtual server,假装自己是服务器,可以接受用户请求,实际上它并不提供服务;后端真正提供服务的叫real server;由于lvs是通过请求报文的目标IP和PORT将其转发至后端主机集群中的某一台主机(根据挑选算法),是工作在第四层网络模型上的,所有也有人把它称为四层交换(layer 4,或者四层路由).

- lvs要附着在netfilter上工作的,lvs并不工作在PREROUTING上,而是工作在INPUT上,强行改变它的流转路程

- lvs: 由两段组成,和iptables类似,ipvsadm/ipvs:
    - 第一段 ipvsadm: 工作在用户空间,让用户写规则,定义谁是集群服务,用什么算法,有多少real server;用户空间的命令行工具,用于管理集群服务: 增删改查.
    - 第二段 ipvs: 把第一段的规则仍在ipvs上,只要在ipvs上有规则,它就能提供服务,其实不管用不用,ipvs都在INPUT那里;工作于内核中netfilter INPUT钩子上.

- lvs支持的协议: TCP,UDP,AH,EST,AH_EST,SCTP等诸多协议

##### lvs的架构和术语
- 术语
    - 负载均衡器(调度器): Director, dispatcher(分发器), Load Balancer;
    - 真正服务器: RS(Real Server);
    - IP名称: 客户端的CIP(Client IP), 负载均衡用来接受客户端请求的VIP(virtual ip), 负载均衡器用来和真正服务器通信的IP叫DIP(Director IP), 真实服务器的IP就叫RIP1(RIP2 ...);

- lvs的架构类型: 调度器如何将用户请求分发给后端Real server服务器的
    1. lvs-nat: 基于多目标dnat方式工作的
    2. lvs-dr(direct routing): 直接路由
    3. lvs-tun(ip tunneling): ip隧道
    4. lvs-fullnat: 非标准类型,有特殊公用,仍属于nat的扩展

- lvs-nat: 多目标的dnat(iptables的dnat机制)

```
大体工作模式: 负载均衡器有两块网卡, 一块面向互联网,一块面向内部的real server(DIP和RIP都一般使用内网地址);
当用户请求到达时,请求源地址是CIP, 目标地址是VIP;如果我们在INPUT那定了规则,
强行转换到了PREROUTING那,这时候源地址不变还是CIP, 目标地址就变成了RIP1的地址(内网地址)
响应报文模式: 从RIP出来的时候, 源地址是RIP1,目标地址是CIP(注意,一定是CIP);到了负载均衡器的时候,
源地址就变成了VIP;

所以这里还要注意:
1) RS和DIP都应该使用私网地址,且RS(真实服务器)的网关一定要指向负载均衡器
2) 请求和响应保温都要经由director转发,极高负载的场景中,director可能会成为系统瓶颈
3) 支持端口映射
4) RS可以使用任意操作系统
5) RS的RIP和Director的DIP必须在同一IP网络(因为RS的网关要指向Director)
```

- lvs-dr: direct routing

```
实现方式: 通过修改请求保温的目标MAC地址进行转发;
场景: RS和Director都只有一块网卡,连接到一台交换机上,客户端请求中间通过层层路由,
到服务器所在的路由器R1的时候,源地址就变成了该路由器R1,而目标地址就是VIP所在的主机的MAC地址

要点: Director收到请求后会从RS中跳一个服务器RS2出来响应请求,RS2会直接响应客户端,
不再经由Director;也就是说只有请求报文经由Director,响应报文是不会经过Director的;
这样就麻烦了,RS2在响应客户端时,就直接跟客户端建立通信了,响应报文的源IP必须得是VIP,
但是实际响应报文并没有经由Director,所以我们就要将所有的RS都再配一个VIP,
具体过程, 当Director收到请求后,会拆开报文头部,封装一个RS的VIP对应的MAC,此时请求报文源IP是CIP,
目标IP是RS的VIP,然后路由器发起广播找到MAC对应的RS,由RS来回应客户端请求.

注意事项:
1) 保证前段路由器将目标IP为VIP的请求报文发送给Director;
    a) 只接把VIP地址和Director的MAC绑定
    b) linux 2.6.5+内核中可以修改RS主机内核的参数
2) RS的RIP可以使用私有地址,也可以使用公网地址;
3) RS和Director一定要在同一个交换机网络上(不要被路由器隔离)
4) 请求报文经由Director调度,但是响应保温一定不能经由Director;不支持端口映射
5) RS指向的网关一定要和RIP在同一网段中,但是RS的网关不能和Director
6) RS的RIP配置在物理网卡上,VIP配置在lo接口的别名上的,且要求从哪个网卡进来的报文必须还从哪个接口出去;
我们的操作是从物理网卡上转发到lo网卡.
7) RS的网关一定不能指向DIP
8) RS可以是大多数的操作系统
```

- lvs-tun: IP隧道机制: 当director收到请求的时候,不修改请求报文的ip首部,而是通过在原有的ip首部之外,再封装一个ip首部.

```
1) Director可以在任何位置,VIP一定是配置在Director上的;
2) Director和各Real Server可以不在同一机房或同一地域;
3) Real Server可以放在任何位置,但是每一个Real Server的RIP必须是公网地址
当Director收到请求报文时,在原来的报文的IP首部(源地址是CIP,目标地址时VIP)外面又封装了一个首部(源地址可
能是DIP,目标地址时挑出来的某一个RIP);RS收到报文拆开一开,
目标地址不是自己,按道理应该转发出去,所以这里RS应该和dr类型相似,在自己的lo接口上再配置一个VIP地址,
当内核发现目标地址时自己的lo接口,于是把里面的报文也拆掉了,
然后把回应报文通过lo接口发送给客户端应用程序.

注意事项:
a) 这时候Director和RS并不需要在同一个网络内,RIP,DIP,vip全得是公网地址
b) RS的网管不能指向Director
c) 请求报文必须经由Director调度,但响应报文必须不能经由才.
d) 要求Director和各RS支持隧道协议,也就是IP首部之外再封装一个首部
e) 互联网上发送的IP报文要有一个MTU(最大传输单元)的概念,标准是1500个字节,突然间又多了一截IP报文,
可能整个报文就超出了1500,然后就可能产生fragment(IP二次切片),二次切片必须
经由的设备允许切片才行,有些设备可能不允许在此切片,那么就会造成报文无法送达;
我们也可以在本地主机设定报文的数量不大于1400.给外面追加的IP报文空间生出来
```
- lvs-fullnat: 能够在请求报文到达时,director通过同事修改请求报文的目标地址和源地址进行转发

```
当Director收到报文时修改报文的源地址为DIP,目标地址为选出的某个RIP,然后响应报文的源地址为RIP,
目标地址为DIP,到了Director,再次修改响应报文,将源地址改为VIP,目标地址改为CIP
注意事项:
1) VIP时公网地址,RIP和DIP时私网地址,二者无需在同一网络中
2) RS接收到的请求报文源地址为DIP,因此要响应给DIP
3) 请求报文和响应报文都必须经由Director
4) 支持端口映射机制
5) RS的操作系统可以使用任意OS
```

#### lvs类型和调度方法

- http协议是无状态的协议,后来我们服务器程序为了能够追踪每一个用户,给每一个客户端发一个id号(cookie),服务器端为了能够追踪用户的行为,要为用户保存session,这样一来就使得服务器应用变成有状态.负载均衡时就不能够任意的分发了,因为协议时无状态的,因此一个用户第一次请求以后,如果我们没有使用长连接,用户端口后,再发请求,就会被识别成新用户,就有可能被调度到其他主机上去了,此前保存的session就丢失了,我们显然不能让这种情况发生;这里我们第一个要解决的问题就是session保持问题
- session保持有多种保持方案(至关重要):
	- session绑定:
		- 源地址hash机制(source ip hash):将来自于同一个用户的请求,我们不做负载均衡,始终定向于同一台Real Server,我们要自行维护一个会话追踪表,根据IP地址追踪每一个请求的客户端.这种机制我们通常称为源地址hash机制(source ip hash),力度过于粗糙
		- cookie hash: 不管你来自于哪个IP,我们在请求报文中插入一个cookie,对于这个cookie,跟http协议中的cookie可以是同一个,也可以不是同一个.就算你不是http协议,我也可以给你cookie.这样就使得,整个交互过程中,每一次客户端请求都拿着这个cookie来,cookie时进程级别的,所以就非常精细了
	- session集群:
		- session绑定有一个严重的问题:万一某个RS宕机了,那么这个主机上的session也就都随它而去了,每一个RS都能同步其他RS上的sesion,也就是说每一个RS都会有进群中所有的session,这也有问题,网络和内存中充斥着大量的session.很浪费资源.
	- session服务器
		- 我们找一个第三方的存储服务器,用来专门存储session.
- lvs scheduler:lvs调度器,其实就是一些算法,能够根据当前后台的负载,来判定下次该挑选谁了,查看当前系统支持的算法:

```
// 其实还有种算法没有列出来,就是NQ
[root@mail ~]# grep -i -A 10 'IPVS' /boot/config-2.6.32-431.el6.x86_64
# IPVS scheduler
CONFIG_IP_VS_RR=m
CONFIG_IP_VS_WRR=m
CONFIG_IP_VS_LC=m
CONFIG_IP_VS_WLC=m
CONFIG_IP_VS_LBLC=m
CONFIG_IP_VS_LBLCR=m
CONFIG_IP_VS_DH=m
CONFIG_IP_VS_SH=m
CONFIG_IP_VS_SED=m
```

- 算法大体上又分为2两种
	- 静态算法: 仅根据算法本身进行调度
		- RR: round robin,轮调
		- WRR: weighted rr,加权RR,能者多劳,权重大的
		- SH: Source Hash, 源IP地址HASH实现session保持的机制,将来源于同一个IP的请求始终调度至同一个RS
		- DH: Destination Hash, 目标地址hash,常用的方法是,不管你的IP是什么,只要你们请求的时同一个目标,就把你定到某一个固定的RS,常用于缓存服务器;还有个案例,我们公司有两个外网出口,内网的所有客户端都要通过这俩接口出去,很有可能1号出口没什么人,2号出口全是人,为了避免这种情况,我们可以做负载均衡,加个调度器,内网中无论哪些客户端访问互联网的时候由调度器,做负载均衡决定从两个出口中的哪一个出去;防火墙有所谓的链接追踪功能，因此一个客户端的请求通过第一个防火墙被负载均衡(调度器A)之后发出去，我们想要它的响应报文依然会通过第一个防火墙进来，不然我们没法实现连接追踪的功能；所以我们需要在两个防火墙外面再安排一个负载均衡调度器B，网内的请求由它统一分发到互联网上去，而响应报文也是由它负责统一调度做DH绑定。
	- 动态方法: 根据算法及各RS的当前负载状态进行调度

        ```
        我们决定挑选一个后端Real Server来进行响应的挑选方式是有一种计算机制的，每一个服务器当前的
        负载(overhead)状态要记录下来,我们可以计算每一台服务器的overhead，负载较小的那台主机就是
        挑出来的那台主机
        ```

		- LC: Least Connection，最小连接数；但是如果我们所有RS的连接数都为0的时候，就从服务器列表中自上而下来分发；Overhead=Actives×256+Inactive, 计算结果较小的即为挑中的主机；
		- WLC: Weighted LC， 加权最少连接,说白了能者多劳，权重大的将承载更多的请求数量，Overhead=(Actives×256+Inactive)÷权重，得值较小的即为挑选出的主机；这种算法也有缺陷：就来了一个请求给谁处理呢？给第一个，第一个权重最小，性能最差，给第一个来处理不妥吧，于是我们就有了SED机制
		- SED：Shortest Expection Delay最短期望延迟; Overhead=(Actives+1×256+Inactive)÷权重，这样值最小的(权重最大的)就会来响应请求；这依然会有问题：比如我们只有两台服务器一个权重是1，一个权重是4，于是第一个请求给了第二台服务器，第二个还是给第二台服务器，第三个还是给了第二台服务器，处理方法其实也简单，每挑出来一台服务器,下一个请求进来时,就把这太服务器排除掉
		- NQ: SED还是有缺陷,比如我们只有两台服务器一个权重是1，一个权重是4，于是第一个请求给了第二台服务器，第二个还是给第二台服务器，第三个还是给了第二台服务器，处理方法其实也简单，每挑出来一台服务器,下一个请求进来时,就把这太服务器排除掉,这就是NQ(Never Queue永不排队)机制;
		- LBLC: Locality-Based LC,基于本地的最少连接,动态的DH算法;只有在实现正向代理,代理本地大量客户端访问互联网,有缓存时才有用;正向代理情形下的Cache Sever调度;这也有一个问题,假如这个服务器上的缓存内容被N多人访问,压力山大,另外的服务器(没有缓存内容)很清闲,那怎么办呢?
		- LBLCR:Locality-Based LC with Replication, 如果像个办法让RS可以互相复制r然后分流一些请求,这就是带复制的Locality-Based LC

#### lvs的调度
- 一个ipvs主机上可以同时定义多个cluster service(多个端口)
- 每一个集群服务上至少应该有1个real server;定义时指明lvs-type,以及lvs-scheduler
##### ipvsadm的用法:大体分两种
1. 管理集群服务,语法
    ```
    //增加|修改集群服务
    ~]# ipvsadm -A|E -t|u|f service-address [-s scheduler]
    //删除集群服务
    ~]# ipvsadm -D -t|u|f service-address
    //清空集群服务和Real Servers
    ~]# ipvsadm -C
    //查询集群服务
    ~]# ipvsadm -L|l
        -n: numeric,基于数字格式显示地址和端口
        -c:显示当前的连接
        --stats (stats:statistics):显示统计数据
        --rate: 显示速率
        --exact: 显示精确值,而不做单位换算
    //注意事项:
    1) service-address:
    	tcp: -t IP:PORT
        udp: -u IP:PORT
        fwm(firewall mark): -f mark(数字)
    2) -s scheduler
    	默认为wlc
    ```

2. 管理集群服务中的RS
    ```
    //增加|修改集群服务中的real servers
    ~]# ipvsadm -a|e -t|u|f service-address -r server-address [-g|i|m] [-w weight]
    //删除集群服务中的real servers
    ~]# ipvsadm -d -t|u|f service-address -r server-address
    //清空集群服务中的real servers
    ~]# ipvsadm -C
    //查询集群服务中的real servers
    ~]# ipvsadm -L|l
        -n: numeric,基于数字格式显示地址和端口
        -c:显示当前的连接
        --stats (stats:statistics):显示统计数据
        --rate: 显示速率
        --exact: 显示精确值,而不做单位换算
    //注意事项:
    1) server-address: Real Server的地址,IP[:PORT]
    2) lvs-type:
    	 -g: gateway,dr模型,也是默认模型
       -i: ipip,tun模型
       -m: masquerade,nat模型
    ```

3. 其他通用命令:

    ```
    //重载
    ~]# ipvsadm -R
    //保存
    ~]# ipvsadm -S
    // 置零计数器
    ~]# ipvsadm -Z [-t|u|f service-address]
    ```

#### lvs-nat的实现
##### 实现环境

- 硬件配置:
    1. Director: CentOS7
    2. n个CentOS6作为Real Servers.
- 实验拓扑结构

	- Director配置情况:

    ```
    // 这个服务器应该有两个网络接口
    一个配置成VIP(172网段bridge eth0,172.16.100.9),另外一个接口(用来配置dip,vmnet2, eth1, 192.168.20.1)
    ```

    - Real Servers配置情况d

    ```
    // RS1的配置情况,只要配置一个RIP就可以了,网关要指向192.168.20.1,提供web服务
    vmnet2,eth0,rip 192.168.20.7
    // RS1的配置情况,只要配置一个RIP就可以了,网关要指向192.168.20.1,提供web服务
    vmnet2,eth0,rip 192.168.20.8
    ```

    - 实现功能:

    ```
    // Director: CentOS7
    1. 启用凉快网卡,在/etc/sysconfig/network-scripts/IFACE_NAME中配置IP地址.
    2. 装在光盘,进入Packages/目录,安装ipvsadmin
	   ~]# rpm -ivh ipvsadm-1.27-7.el7.x86_64.rpm		//或者通过yum安装
    // RS1: node1
    1. 配置IP地址为192.168.20.7,将网关指向192.168.20.1
    // RS1: node2
    1. 配置IP地址为192.168.20.8,将网关指向192.168.20.1

    // 操作过程
    1. Director的防火墙规则一定要是清空了的,因为iptables和ipvs是很难共存的
    2. Director的ip_forward要打开,
        编辑/etc/sysctl.conf,加上net.ipv4.ip_forward = 1,然后可以通过sysctl -p查看
    3. Director上定义规则:
        ~]# ipvsadm -A -t 172.16.100.9:80 -s rr
        ~]# ipvsadm -L -n   //查看规则-n表示不反解
    4. 添加real servers
        ~]# ipvsadm -a -t 172.16.100.9:80 -r 192.168.20.7 -m
        ~]# ipvsadm -a -t 172.16.100.9:80 -r 192.168.20.8 -m
    5. 查看Real Servers
        ~]# ipvsadm -L -n //可以查看当前存在的Real Servers
    // 此时我们访问172.116.100.9时就是轮询RS1和RS2
    6. 我们还可以通过输入重定向保存规则
        ~]# ipvsadm -S > /etc/sysconfig/ipvsadm
    7. 恢复规则也很简单
        ~]# ipvsadm -R < /etc/sysconfig/ipvsadm
    ```

    - 修改编辑规则

    ```
    // 修改rr调度器为sh
    ~]# ipvsadm -E -t 172.16.100.9:80 -s sh //-E修改集群服务
    ~]# ipvsadm -L -n   //查看规则,会发现调度器已经变成了sh
    // 修改realserve 通过-e来修改real server的端口,比如我们real server的httpd还监听了8080端口
    ~]# ipvsadm -e -t 172.16.100.9:80 -r 192.168.20.7:8080 -m
    // 或者我们直接编辑/etc/sysconfig/ipvsadm, 修改http(这个是反解的结果)为8080,然后再
    通过ipvsadm -R < /etc/sysconfig/ipvsadm
    ```

    - 删除real server或者集群服务

    ```
    // 删除real server
    ~]# ipvsadm -d -t 172.16.100.9:80 -r 192.168.20.7:8080  //这就删除了192.168.20.7这台real servers
    // 删除集群服务
    ~]# ipvsadm -D -t 172.16.100.9:80   //就删除了集群服务
    ```

- http负载均衡注意点:

  ```
  多台real servers必须使用同一个证书同一个私钥,ipvs模式才这么做,而我们生产环境中大部分用的是
  haproxy或者nginx来做负载均衡,我们会在client和director之间用https,
  而director和rs之间通信还是http.
  ```

##### lvs-dr的实现

- 这种模式实现的方法级可能出现的问题: 客户端发起请求到vip,但是由于Director和Real Servers都配置了vip,而我们要求请求报文一定要发送给Director,响应报文也只能是vip响应(这就是RS要配置vip的原因);
  1. 路由器直接绑定vip和Director的MAC地址,我们可能启用了高可用模式(准备了多台调度器Director),但是我们路由(可能是网络供应商那边的路由)接进来的vip(相对公网IP,比如无锡电信给的一个IP)只绑定一台Director的MAC地址,一旦这一台Director挂了怎么办?更何况,我们就算想修改vip的绑定,也得有路由器的管理权限,如果直接从运营商接入的话是不现实的!!
  2. 我们可以在RS上设置ARPtables规则,明确规定,当目标地址是vip的广播地址arp请求时,real servers不予响应(要么响应报文不让出去,要么请求报文不让进来);这个方法的问题是,Real Servers必须支持arptables,局限性太大(windows平台就不行)
  3. linux内核中有一种法则,我们在给real server配置ip的时候,把vip配置在lo接口的别名上(而不是物理网卡上);我们还可以在内核中配置一种法则(简单来说就是内核参数),如果我们RS接受到arp请求,即使目标地址是vip,我们的RS也不会响应

- 对dr模型来讲,它还有一个工作机制,请求报文到达Director时,Director再转发至Real Server时,是通过修改MAC地址实现的.
  ```
  请求报文从路由器到Director时,源MAC是路由器接口的MAC,目标MAC是Director的VIP所在物理网卡的MAC地址;Director要挑选
  一个Real Server来响应,但是我们配置RealServer的时候给的是IP地址,而不是MAC,所以此时,我们的Director还要广播(解析请求)
  获得每一个RIP对应的MAC地址,才能调度时,修改目标MAC对应的那台主机的IP地址;然后RS物理网卡收进去以后转发给lo网络接口,
  再由lo网络接口发送给用户空间的服务器程序;响应的时候,我们要在RS上设立一个路由,要求通过lo接口别名进来,还得从那个接口别名出去;
  虽然源地址是VIP,但是报文还得从物理网卡接口出去
  ```

##### 演示过程

- 主机配置情况
  - Director配置: Bridge,eth0, DIP:172.16.100.9, VIP: 172.16.100.10
  - RS1配置: Bridge,eth0, RIP 172.16.100.21, lo0(lo别名): VIP 172.16.100.10
  - RS2配置: Bridge,eth0, RIP 172.16.100.22, lo0(lo别名): VIP 172.16.100.10

- arp_announce: 广播,通告别人;把自己的信息告诉别人; 这个就是定义是否通告给别人和接受别人的通告;有三个参数
  - 0: 默认参数,我有什么地址都统统通告出去;___如果不做Realserver了还得改回为0___
  - 1: __尽量__ 避免向非本网络透露非本网络接口,也就是只在同一个网络中的地址
  - 2: 只能用最佳本地地址向本网络内通告;__最好用这个__

- arp_ignore: 但是有些人是后来的没听到广播,或者先来的人缓存到期了,那就只好等别人的被动请求了;这个就是用来定义是否响应别人的ARP请求常用有9个参数:___如果不做Realserver了还得改回为0___
  - 0: 默认参数,不给谁发的请求,我把我所有的地址都告诉他
  - 1: 请求报文从哪个接口进入的,就使用那个接口响应,__就用这个参数__; 其他的就不介绍了

- Director配置
  ```
  1. 用setup配置DIP:172.16.100.9
  2. 配置VIP:172.16.100.10
    ~]# ifconfig eth0:0 172.16.100.10/32 broadcast 172.16.100.10 up //子网掩码为32且只广播给自己
  3. 定义从哪个接口进来从那个接口出去
    ~]# route add -host 172.16.100.10 dev eth0:0
  4. 当Real Server的配置中前五步都完成以后,在Director上添加ipvsadm规则
    ~]# ipvsadm -C  //清除当前的所有规则
    ~]# ipvsadm -A -t 172.16.100.10:80 -s rr
    ~]# ipvsadm -A -t 172.16.100.10:80 -r 172.16.100.21 -g  //添加第一个RealServer
    ~]# ipvsadm -A -t 172.16.100.10:80 -r 172.16.100.22 -g  //添加第二个RealServer
  ```

- Real Server配置: 一定要先添加内核参数再配ip
  ```
  1. 用setup配置物理RIP地址
  2. 所有Real Servers都配置VIP:172.16.100.10
  3. 配置arp_ignore和arp_announce参数(all 可以换成对应的网卡名字)
    ~]# echo 1 > /proc/sys/net/ipv4/conf/all/arp_ignore
    ~]# echo 1 > /proc/sys/net/ipv4/conf/eth0/arp_ignore
    ~]# echo 2 > /proc/sys/net/ipv4/conf/all/arp_announce
    ~]# echo 2 > /proc/sys/net/ipv4/conf/eth0/arp_announce
  4. 在lo网络接口上配置VIP:
    ~]# ifconfig lo:0 172.16.100.10/32 broadcast 172.16.100.10 up //仅仅广播给自己
  5. 定义从哪个接口进来从那个接口出去
    ~]# route add -host 172.16.100.10 dev lo:0
  ```
